{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARIDA Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to set up a folder for unified visualisation and qgis qml visualisation because the quality is the true definition of terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4143\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "all_tifs = glob(\"datasets/MARIDA_sentinel-2/patches/**/*.tif\", recursive=True)\n",
    "print(len(all_tifs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import re\n",
    "\n",
    "# Compile a regex pattern to match \"C_{number}.tif\"\n",
    "pattern = re.compile(r\"C_\\d+\\_conf.tif$\")\n",
    "\n",
    "# Filter files using the regex pattern\n",
    "filtered_files = [file for file in all_tifs if pattern.search(file)]\n",
    "\n",
    "for file in filtered_files:\n",
    "    shutil.copy(file, \"datasets/MARIDA_sentinel-2/only_confs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(len(os.listdir(\"datasets/MARIDA_sentinel-2/only_confs/\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename files nicely with 6 digit indexing and keep the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "base_dir = \"datasets/MARIDA_sentinel-2/\"\n",
    "confs_dir = os.path.join(base_dir, \"only_confs\")\n",
    "cls_dir = os.path.join(base_dir, \"only_cls\")\n",
    "visuals_dir = os.path.join(base_dir, \"only_visuals\")\n",
    "shape_files = os.path.join(base_dir, \"shapefiles\")\n",
    "\n",
    "confs_files = glob(os.path.join(confs_dir, \"*.tif\"))\n",
    "cls_files = glob(os.path.join(cls_dir, \"*.tif\"))\n",
    "visuals_files = glob(os.path.join(visuals_dir, \"*.tif\"))\n",
    "shape_files = glob(os.path.join(shape_files, \"*.shp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In planet data, make sure that filename matches annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def fix_path(path: str) -> str:\n",
    "    return path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "labels_dir = fix_path(\"datasets\\Planet\\labels\")\n",
    "source_dir = fix_path(\"datasets\\Planet\\source\")\n",
    "\n",
    "labels = sorted([os.path.splitext(i)[0] for i in glob(os.path.join(labels_dir, \"*.geojson\"))])\n",
    "source = sorted([os.path.splitext(i)[0] for i in glob(os.path.join(source_dir, \"*.jpg\"))])\n",
    "\n",
    "# Compare the two lists and make sure that the list items are the same\n",
    "print(all([os.path.basename(i) == os.path.basename(j) for i, j in zip(labels, source)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ananya's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/Planet/labels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def fix_path(path: str) -> str:\n",
    "    return path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "images_dir = fix_path(\"datasets\\Planet\\source\")\n",
    "labels_dir = fix_path(\"datasets\\Planet\\labels\")\n",
    "output_dir = fix_path(\"datasets\\Planet\\yolo_labels\")\n",
    "\n",
    "class_map = {\n",
    "    \"marine_debris\": 0  # Add more classes if needed, with unique IDs\n",
    "}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to normalize coordinates\n",
    "def normalize_bbox(bbox, img_width, img_height):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_center = (x_min + x_max) / 2 / img_width\n",
    "    y_center = (y_min + y_max) / 2 / img_height\n",
    "    width = (x_max - x_min) / img_width\n",
    "    height = (y_max - y_min) / img_height\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "# Loop through GeoJSON files\n",
    "for filename in os.listdir(labels_dir):\n",
    "    if filename.endswith(\".geojson\"):\n",
    "        geojson_path = os.path.join(labels_dir, filename)\n",
    "\n",
    "        # Load GeoJSON\n",
    "        with open(geojson_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract image filename\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        image_path = os.path.join(images_dir, base_name + \".jpg\")  # Update extension if needed\n",
    "\n",
    "        # Check if image exists\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found for {base_name}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Get image dimensions\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "\n",
    "        # Create YOLO label file\n",
    "        yolo_file_path = os.path.join(output_dir, base_name + \".txt\")\n",
    "        with open(yolo_file_path, \"w\") as yolo_file:\n",
    "            # Iterate through features\n",
    "            for _, row in data.iterrows():\n",
    "                # Access the 'name' column directly\n",
    "                class_name = row.get(\"name\", None)  # Replace 'name' with the exact column name if different\n",
    "                if class_name not in class_map:\n",
    "                    print(f\"Class {class_name} not in class_map, skipping...\")\n",
    "                    continue\n",
    "\n",
    "                class_id = class_map[class_name]\n",
    "                geometry = row.geometry\n",
    "\n",
    "                # Get bounding box\n",
    "                if geometry and geometry.is_valid:\n",
    "                    bbox = geometry.bounds  # (minx, miny, maxx, maxy)\n",
    "                    x_center, y_center, width, height = normalize_bbox(bbox, img_width, img_height)\n",
    "\n",
    "                    # Write to YOLO file\n",
    "                    yolo_file.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "print(\"Conversion complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to check for all the unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0}\n",
      "[[-87.623357, 15.975562], [-87.621791, 15.975562], [-87.621791, 15.974811], [-87.623357, 15.974811]]\n",
      "[[-87.621907, 15.977172621632805], [-87.6214599609375, 15.977172621632805], [-87.6214599609375, 15.975994], [-87.621907, 15.975994]]\n",
      "[[-87.622971, 15.961329081596643], [-87.6214599609375, 15.961329081596643], [-87.6214599609375, 15.958825], [-87.622971, 15.958825]]\n",
      "[[-87.623906, 15.956811], [-87.623906, 15.95604762305055], [-87.626953125, 15.95604762305055], [-87.626953125, 15.956811]]\n",
      "[[-87.624236, 15.951041], [-87.623361, 15.951041], [-87.623361, 15.950766025306109], [-87.624236, 15.950766025306109]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Read sample file as a dictionary\n",
    "labels_path = fix_path(r\"datasets\\Planet\\labels\")\n",
    "label_files = glob(os.path.join(labels_path, \"*.geojson\"))\n",
    "\n",
    "class_map = {\n",
    "    \"marine_debris\": 0  # Add more classes if needed, with unique IDs\n",
    "}\n",
    "\n",
    "labels = []\n",
    "coords = []\n",
    "\n",
    "# Indiscriminately loop through all GeoJSON files without separating by file for exploratory purposes\n",
    "for file in label_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for feature in data[\"features\"]:\n",
    "        label = class_map[feature[\"properties\"][\"name\"]]\n",
    "        labels.append(label)\n",
    "\n",
    "        coordinates = feature[\"geometry\"][\"coordinates\"][0][:-1]\n",
    "        coords.append(coordinates)\n",
    "\n",
    "print(set(labels))\n",
    "print(*coords[:5], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um, seems like the coords are in order of top_left, top_right, bottom_right, bottom_left. Realized that each point is in [longitude, latitude] format, and I checked all this via opening the image on QGIS and checking coord points. This complicates things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect metadata of the image for any border coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-87.626953125 15.971891580928972 -87.6214599609375 15.977172621632805\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "\n",
    "test_tif = fix_path(r\"datasets\\Planet\\source\\20160928_153233_0e16_16816-29821-16.tif\")\n",
    "\n",
    "with rasterio.open(test_tif) as src:\n",
    "    # print(src.profile)\n",
    "    bounds = src.bounds\n",
    "\n",
    "left, bottom, right, top = bounds\n",
    "print(left, bottom, right, top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hmm, image metadata for coords is ridiculously complicated. Longitude latitude needs to be changed to row, column format. Then the order needs to be changed. Then top-left bottom-right needs to be obtained. Then normalized.\n",
    "#### Let's try numpy files, since they're also given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[163, 74, 244, 118, 1], [230, 0, 255, 61, 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read npy file\n",
    "test_file = fix_path(r\"datasets\\Planet\\labels\\20160928_153233_0e16_16816-29821-16.npy\")\n",
    "data: np.ndarray = np.load(test_file, allow_pickle=True)\n",
    "\n",
    "print(data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, npy files work better? documentation.pdf in the datasets/Planet directory says it's perfectly in <b>[xmin, ymin, xmax, ymax, class_id]</b> form.  I need to make sure there's one file per jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def fix_path(path: str) -> str:\n",
    "    return path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "labels_dir = fix_path(\"datasets\\Planet\\labels\")\n",
    "source_dir = fix_path(\"datasets\\Planet\\source\")\n",
    "\n",
    "labels = sorted([os.path.splitext(i)[0] for i in glob(os.path.join(labels_dir, \"*.npy\"))])\n",
    "source = sorted([os.path.splitext(i)[0] for i in glob(os.path.join(source_dir, \"*.jpg\"))])\n",
    "\n",
    "# Compare the two lists and make sure that the list items are the same\n",
    "print(all([os.path.basename(i) == os.path.basename(j) for i, j in zip(labels, source)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flawless. Now, we just need to move the npy info to a txt, normalize the coords and re-order to make it [class_id, xcenter, y_center, width, height]. All the code is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marine_debris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
